"""
è·¯å¾„è¯„åˆ†æ‰©å±•ç®—æ³•

åŸºäºå›¾è·¯å¾„ä¼ æ’­çš„è®°å¿†æ£€ç´¢ä¼˜åŒ–æ–¹æ¡ˆï¼š
1. ä»å‘é‡æœç´¢çš„TopKèŠ‚ç‚¹å‡ºå‘ï¼Œåˆ›å»ºåˆå§‹è·¯å¾„
2. æ²¿è¾¹æ‰©å±•è·¯å¾„ï¼Œåˆ†æ•°é€šè¿‡è¾¹æƒé‡å’ŒèŠ‚ç‚¹ç›¸ä¼¼åº¦ä¼ æ’­
3. è·¯å¾„ç›¸é‡æ—¶åˆå¹¶ï¼Œç›¸äº¤æ—¶å‰ªæ
4. æœ€ç»ˆæ ¹æ®è·¯å¾„è´¨é‡å¯¹è®°å¿†è¯„åˆ†

æ ¸å¿ƒç‰¹æ€§ï¼š
- æŒ‡æ•°è¡°å‡çš„åˆ†æ•°ä¼ æ’­
- åŠ¨æ€åˆ†å‰æ•°é‡æ§åˆ¶
- è·¯å¾„åˆå¹¶ä¸å‰ªæä¼˜åŒ–
- å¤šç»´åº¦æœ€ç»ˆè¯„åˆ†
"""

import asyncio
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import TYPE_CHECKING, Any

from src.common.logger import get_logger
from src.memory_graph.utils.similarity import cosine_similarity

if TYPE_CHECKING:
    import numpy as np

    from src.memory_graph.models import Memory
    from src.memory_graph.storage.graph_store import GraphStore
    from src.memory_graph.storage.vector_store import VectorStore

logger = get_logger(__name__)


@dataclass
class Path:
    """è¡¨ç¤ºä¸€æ¡è·¯å¾„"""

    nodes: list[str] = field(default_factory=list)  # èŠ‚ç‚¹IDåºåˆ—
    edges: list[Any] = field(default_factory=list)  # è¾¹åºåˆ—
    score: float = 0.0  # å½“å‰è·¯å¾„åˆ†æ•°
    depth: int = 0  # è·¯å¾„æ·±åº¦
    parent: "Path | None" = None  # çˆ¶è·¯å¾„ï¼ˆç”¨äºè¿½è¸ªï¼‰
    is_merged: bool = False  # æ˜¯å¦ä¸ºåˆå¹¶è·¯å¾„
    merged_from: list["Path"] = field(default_factory=list)  # åˆå¹¶æ¥æºè·¯å¾„

    def __hash__(self):
        """ä½¿è·¯å¾„å¯å“ˆå¸Œï¼ˆåŸºäºèŠ‚ç‚¹åºåˆ—ï¼‰"""
        return hash(tuple(self.nodes))

    def get_leaf_node(self) -> str | None:
        """è·å–å¶å­èŠ‚ç‚¹ï¼ˆè·¯å¾„ç»ˆç‚¹ï¼‰"""
        return self.nodes[-1] if self.nodes else None

    def contains_node(self, node_id: str) -> bool:
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦åŒ…å«æŸä¸ªèŠ‚ç‚¹"""
        return node_id in self.nodes


@dataclass
class PathExpansionConfig:
    """è·¯å¾„æ‰©å±•é…ç½®"""

    max_hops: int = 2  # æœ€å¤§è·³æ•°
    damping_factor: float = 0.85  # è¡°å‡å› å­ï¼ˆPageRanké£æ ¼ï¼‰
    max_branches_per_node: int = 10  # æ¯èŠ‚ç‚¹æœ€å¤§åˆ†å‰æ•°
    path_merge_strategy: str = "weighted_geometric"  # è·¯å¾„åˆå¹¶ç­–ç•¥: weighted_geometric, max_bonus
    pruning_threshold: float = 0.9  # å‰ªæé˜ˆå€¼ï¼ˆæ–°è·¯å¾„åˆ†æ•°éœ€è¾¾åˆ°å·²æœ‰è·¯å¾„çš„90%ï¼‰
    high_score_threshold: float = 0.7  # é«˜åˆ†è·¯å¾„é˜ˆå€¼
    medium_score_threshold: float = 0.4  # ä¸­åˆ†è·¯å¾„é˜ˆå€¼
    max_active_paths: int = 1000  # æœ€å¤§æ´»è·ƒè·¯å¾„æ•°ï¼ˆé˜²æ­¢çˆ†ç‚¸ï¼‰
    top_paths_retain: int = 500  # è¶…é™æ—¶ä¿ç•™çš„topè·¯å¾„æ•°

    # è¾¹ç±»å‹æƒé‡é…ç½®
    edge_type_weights: dict[str, float] = field(
        default_factory=lambda: {
            "REFERENCE": 1.3,  # å¼•ç”¨å…³ç³»æƒé‡æœ€é«˜
            "ATTRIBUTE": 1.2,  # å±æ€§å…³ç³»
            "HAS_PROPERTY": 1.2,  # å±æ€§å…³ç³»
            "RELATION": 0.9,  # ä¸€èˆ¬å…³ç³»é€‚ä¸­é™æƒ
            "TEMPORAL": 0.7,  # æ—¶é—´å…³ç³»é™æƒ
            "DEFAULT": 1.0,  # é»˜è®¤æƒé‡
        }
    )

    # æœ€ç»ˆè¯„åˆ†æƒé‡
    final_scoring_weights: dict[str, float] = field(
        default_factory=lambda: {
            "path_score": 0.50,  # è·¯å¾„åˆ†æ•°å 50%
            "importance": 0.30,  # é‡è¦æ€§å 30%
            "recency": 0.20,  # æ—¶æ•ˆæ€§å 20%
        }
    )


class PathScoreExpansion:
    """è·¯å¾„è¯„åˆ†æ‰©å±•ç®—æ³•å®ç°"""

    def __init__(
        self,
        graph_store: "GraphStore",
        vector_store: "VectorStore",
        config: PathExpansionConfig | None = None,
    ):
        """
        åˆå§‹åŒ–è·¯å¾„æ‰©å±•å™¨

        Args:
            graph_store: å›¾å­˜å‚¨
            vector_store: å‘é‡å­˜å‚¨
            config: æ‰©å±•é…ç½®
        """
        self.graph_store = graph_store
        self.vector_store = vector_store
        self.config = config or PathExpansionConfig()
        self.prefer_node_types: list[str] = []  # ğŸ†• åå¥½èŠ‚ç‚¹ç±»å‹

        logger.info(
            f"PathScoreExpansion åˆå§‹åŒ–: max_hops={self.config.max_hops}, "
            f"damping={self.config.damping_factor}, "
            f"merge_strategy={self.config.path_merge_strategy}"
        )

    async def expand_with_path_scoring(
        self,
        initial_nodes: list[tuple[str, float, dict[str, Any]]],  # (node_id, score, metadata)
        query_embedding: "np.ndarray | None",
        top_k: int = 20,
        prefer_node_types: list[str] | None = None,  # ğŸ†• åå¥½èŠ‚ç‚¹ç±»å‹
    ) -> list[tuple[Any, float, list[Path]]]:
        """
        ä½¿ç”¨è·¯å¾„è¯„åˆ†è¿›è¡Œå›¾æ‰©å±•

        Args:
            initial_nodes: åˆå§‹èŠ‚ç‚¹åˆ—è¡¨ï¼ˆæ¥è‡ªå‘é‡æœç´¢ï¼‰
            query_embedding: æŸ¥è¯¢å‘é‡ï¼ˆç”¨äºè®¡ç®—èŠ‚ç‚¹ç›¸ä¼¼åº¦ï¼‰
            top_k: è¿”å›çš„topè®°å¿†æ•°é‡
            prefer_node_types: åå¥½èŠ‚ç‚¹ç±»å‹åˆ—è¡¨ï¼ˆç”±LLMè¯†åˆ«ï¼‰ï¼Œå¦‚ ["EVENT", "ENTITY"]

        Returns:
            [(Memory, final_score, contributing_paths), ...]
        """
        start_time = time.time()

        if not initial_nodes:
            logger.warning("åˆå§‹èŠ‚ç‚¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè·¯å¾„æ‰©å±•")
            return []

        # ä¿å­˜åå¥½ç±»å‹
        self.prefer_node_types = prefer_node_types or []
        if self.prefer_node_types:
            logger.info(f"ğŸ¯ åå¥½èŠ‚ç‚¹ç±»å‹: {self.prefer_node_types}")

        # 1. åˆå§‹åŒ–è·¯å¾„
        active_paths = []
        best_score_to_node: dict[str, float] = {}  # è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„æœ€ä½³åˆ°è¾¾åˆ†æ•°

        for node_id, score, metadata in initial_nodes:
            path = Path(nodes=[node_id], edges=[], score=score, depth=0)
            active_paths.append(path)
            best_score_to_node[node_id] = score

        logger.info(f"ğŸš€ è·¯å¾„æ‰©å±•å¼€å§‹: {len(active_paths)} æ¡åˆå§‹è·¯å¾„")

        # 2. å¤šè·³æ‰©å±•
        hop_stats = []  # æ¯è·³ç»Ÿè®¡ä¿¡æ¯

        for hop in range(self.config.max_hops):
            hop_start = time.time()
            next_paths = []
            branches_created = 0
            paths_merged = 0
            paths_pruned = 0

            for path in active_paths:
                current_node = path.get_leaf_node()
                if not current_node:
                    continue

                # è·å–æ’åºåçš„é‚»å±…è¾¹
                neighbor_edges = await self._get_sorted_neighbor_edges(current_node)

                # åŠ¨æ€è®¡ç®—æœ€å¤§åˆ†å‰æ•°
                max_branches = self._calculate_max_branches(path.score)
                branch_count = 0

                for edge in neighbor_edges[:max_branches]:
                    next_node = edge.target_id if edge.source_id == current_node else edge.source_id

                    # é¿å…ç¯è·¯
                    if path.contains_node(next_node):
                        continue

                    # è®¡ç®—æ–°è·¯å¾„åˆ†æ•°
                    edge_weight = self._get_edge_weight(edge)
                    node_score = await self._get_node_score(next_node, query_embedding)

                    new_score = self._calculate_path_score(
                        old_score=path.score,
                        edge_weight=edge_weight,
                        node_score=node_score,
                        depth=hop + 1,
                    )

                    # å‰ªæï¼šå¦‚æœåˆ°è¾¾è¯¥èŠ‚ç‚¹çš„åˆ†æ•°è¿œä½äºå·²æœ‰æœ€ä¼˜è·¯å¾„ï¼Œè·³è¿‡
                    if next_node in best_score_to_node:
                        if new_score < best_score_to_node[next_node] * self.config.pruning_threshold:
                            paths_pruned += 1
                            continue

                    # æ›´æ–°æœ€ä½³åˆ†æ•°
                    best_score_to_node[next_node] = max(best_score_to_node.get(next_node, 0), new_score)

                    # åˆ›å»ºæ–°è·¯å¾„
                    new_path = Path(
                        nodes=path.nodes + [next_node],
                        edges=path.edges + [edge],
                        score=new_score,
                        depth=hop + 1,
                        parent=path,
                    )

                    # å°è¯•è·¯å¾„åˆå¹¶
                    merged_path = self._try_merge_paths(new_path, next_paths)
                    if merged_path:
                        next_paths.append(merged_path)
                        paths_merged += 1
                    else:
                        next_paths.append(new_path)

                    branches_created += 1
                    branch_count += 1

                    if branch_count >= max_branches:
                        break

                # è®©å‡ºæ§åˆ¶æƒ
                if len(next_paths) % 100 == 0:
                    await asyncio.sleep(0)

            # è·¯å¾„æ•°é‡æ§åˆ¶ï¼šå¦‚æœçˆ†ç‚¸æ€§å¢é•¿ï¼Œä¿ç•™é«˜åˆ†è·¯å¾„
            if len(next_paths) > self.config.max_active_paths:
                logger.warning(
                    f"âš ï¸  è·¯å¾„æ•°é‡è¶…é™ ({len(next_paths)} > {self.config.max_active_paths})ï¼Œ"
                    f"ä¿ç•™ top {self.config.top_paths_retain}"
                )
                next_paths = sorted(next_paths, key=lambda p: p.score, reverse=True)[
                    : self.config.top_paths_retain
                ]

            active_paths = next_paths

            hop_time = time.time() - hop_start
            hop_stats.append(
                {
                    "hop": hop + 1,
                    "paths": len(active_paths),
                    "branches": branches_created,
                    "merged": paths_merged,
                    "pruned": paths_pruned,
                    "time": hop_time,
                }
            )

            logger.debug(
                f"  Hop {hop+1}/{self.config.max_hops}: "
                f"{len(active_paths)} æ¡è·¯å¾„, "
                f"{branches_created} åˆ†å‰, "
                f"{paths_merged} åˆå¹¶, "
                f"{paths_pruned} å‰ªæ, "
                f"{hop_time:.3f}s"
            )

            # æ—©åœï¼šå¦‚æœæ²¡æœ‰æ–°è·¯å¾„
            if not active_paths:
                logger.info(f"â¹ï¸  æå‰åœæ­¢ï¼šç¬¬ {hop+1} è·³æ— æ–°è·¯å¾„")
                break

        # 3. æå–å¶å­è·¯å¾„ï¼ˆæœ€å°å­è·¯å¾„ï¼‰
        leaf_paths = self._extract_leaf_paths(active_paths)
        logger.info(f"ğŸ“Š æå– {len(leaf_paths)} æ¡å¶å­è·¯å¾„")

        # 4. è·¯å¾„åˆ°è®°å¿†çš„æ˜ å°„
        memory_paths = await self._map_paths_to_memories(leaf_paths)
        logger.info(f"ğŸ”— æ˜ å°„åˆ° {len(memory_paths)} æ¡å€™é€‰è®°å¿†")

        # 5. æœ€ç»ˆè¯„åˆ†
        scored_memories = await self._final_scoring(memory_paths)

        # 6. æ’åºå¹¶è¿”å›TopK
        scored_memories.sort(key=lambda x: x[1], reverse=True)
        result = scored_memories[:top_k]

        elapsed = time.time() - start_time
        logger.info(
            f"âœ… è·¯å¾„æ‰©å±•å®Œæˆ: {len(initial_nodes)} ä¸ªåˆå§‹èŠ‚ç‚¹ â†’ "
            f"{len(result)} æ¡è®°å¿† (è€—æ—¶ {elapsed:.3f}s)"
        )

        # è¾“å‡ºæ¯è·³ç»Ÿè®¡
        for stat in hop_stats:
            logger.debug(
                f"  ç»Ÿè®¡ Hop{stat['hop']}: {stat['paths']}è·¯å¾„, "
                f"{stat['branches']}åˆ†å‰, {stat['merged']}åˆå¹¶, "
                f"{stat['pruned']}å‰ªæ, {stat['time']:.3f}s"
            )

        return result

    async def _get_sorted_neighbor_edges(self, node_id: str) -> list[Any]:
        """
        è·å–èŠ‚ç‚¹çš„æ’åºé‚»å±…è¾¹ï¼ˆæŒ‰è¾¹æƒé‡æ’åºï¼‰

        Args:
            node_id: èŠ‚ç‚¹ID

        Returns:
            æ’åºåçš„è¾¹åˆ—è¡¨
        """
        edges = []

        # ä»å›¾å­˜å‚¨ä¸­è·å–ä¸è¯¥èŠ‚ç‚¹ç›¸å…³çš„æ‰€æœ‰è¾¹
        # éœ€è¦éå†æ‰€æœ‰è®°å¿†æ‰¾åˆ°åŒ…å«è¯¥èŠ‚ç‚¹çš„è¾¹
        for memory_id in self.graph_store.node_to_memories.get(node_id, []):
            memory = self.graph_store.get_memory_by_id(memory_id)
            if memory:
                for edge in memory.edges:
                    if edge.source_id == node_id or edge.target_id == node_id:
                        edges.append(edge)

        # å»é‡ï¼ˆåŒä¸€æ¡è¾¹å¯èƒ½å‡ºç°å¤šæ¬¡ï¼‰
        unique_edges = list({(e.source_id, e.target_id, e.edge_type): e for e in edges}.values())

        # æŒ‰è¾¹æƒé‡æ’åº
        unique_edges.sort(key=lambda e: self._get_edge_weight(e), reverse=True)

        return unique_edges

    def _get_edge_weight(self, edge: Any) -> float:
        """
        è·å–è¾¹çš„æƒé‡

        Args:
            edge: è¾¹å¯¹è±¡

        Returns:
            è¾¹æƒé‡
        """
        # åŸºç¡€æƒé‡ï¼šè¾¹è‡ªèº«çš„é‡è¦æ€§
        base_weight = getattr(edge, "importance", 0.5)

        # è¾¹ç±»å‹æƒé‡
        edge_type_str = edge.edge_type.value if hasattr(edge.edge_type, "value") else str(edge.edge_type)
        type_weight = self.config.edge_type_weights.get(edge_type_str, self.config.edge_type_weights["DEFAULT"])

        # ç»¼åˆæƒé‡
        return base_weight * type_weight

    async def _get_node_score(self, node_id: str, query_embedding: "np.ndarray | None") -> float:
        """
        è·å–èŠ‚ç‚¹åˆ†æ•°ï¼ˆåŸºäºä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦ + åå¥½ç±»å‹åŠ æˆï¼‰

        Args:
            node_id: èŠ‚ç‚¹ID
            query_embedding: æŸ¥è¯¢å‘é‡

        Returns:
            èŠ‚ç‚¹åˆ†æ•°ï¼ˆ0.0-1.0ï¼Œåå¥½ç±»å‹èŠ‚ç‚¹å¯èƒ½è¶…è¿‡1.0ï¼‰
        """
        # ä»å‘é‡å­˜å‚¨è·å–èŠ‚ç‚¹æ•°æ®
        node_data = await self.vector_store.get_node_by_id(node_id)
        
        if query_embedding is None:
            base_score = 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
        else:
            if not node_data or "embedding" not in node_data:
                base_score = 0.3  # æ— å‘é‡çš„èŠ‚ç‚¹ç»™ä½åˆ†
            else:
                node_embedding = node_data["embedding"]
                similarity = cosine_similarity(query_embedding, node_embedding)
                base_score = max(0.0, min(1.0, similarity))  # é™åˆ¶åœ¨[0, 1]

        # ğŸ†• åå¥½ç±»å‹åŠ æˆ
        if self.prefer_node_types and node_data:
            metadata = node_data.get("metadata", {})
            node_type = metadata.get("node_type")
            if node_type and node_type in self.prefer_node_types:
                # ç»™äºˆ20%çš„åˆ†æ•°åŠ æˆ
                bonus = base_score * 0.2
                logger.debug(f"èŠ‚ç‚¹ {node_id[:8]} ç±»å‹ {node_type} åŒ¹é…åå¥½ï¼ŒåŠ æˆ {bonus:.3f}")
                return base_score + bonus

        return base_score

    def _calculate_path_score(self, old_score: float, edge_weight: float, node_score: float, depth: int) -> float:
        """
        è®¡ç®—è·¯å¾„åˆ†æ•°ï¼ˆæ ¸å¿ƒå…¬å¼ï¼‰

        ä½¿ç”¨æŒ‡æ•°è¡°å‡ + è¾¹æƒé‡ä¼ æ’­ + èŠ‚ç‚¹åˆ†æ•°æ³¨å…¥

        Args:
            old_score: æ—§è·¯å¾„åˆ†æ•°
            edge_weight: è¾¹æƒé‡
            node_score: æ–°èŠ‚ç‚¹åˆ†æ•°
            depth: å½“å‰æ·±åº¦

        Returns:
            æ–°è·¯å¾„åˆ†æ•°
        """
        # æŒ‡æ•°è¡°å‡å› å­
        decay = self.config.damping_factor**depth

        # ä¼ æ’­åˆ†æ•°ï¼šæ—§åˆ†æ•° Ã— è¾¹æƒé‡ Ã— è¡°å‡
        propagated_score = old_score * edge_weight * decay

        # æ–°é²œåˆ†æ•°ï¼šèŠ‚ç‚¹åˆ†æ•° Ã— (1 - è¡°å‡)
        fresh_score = node_score * (1 - decay)

        return propagated_score + fresh_score

    def _calculate_max_branches(self, path_score: float) -> int:
        """
        åŠ¨æ€è®¡ç®—æœ€å¤§åˆ†å‰æ•°

        Args:
            path_score: è·¯å¾„åˆ†æ•°

        Returns:
            æœ€å¤§åˆ†å‰æ•°
        """
        if path_score > self.config.high_score_threshold:
            return int(self.config.max_branches_per_node * 1.5)  # é«˜åˆ†è·¯å¾„å¤šæ¢ç´¢
        elif path_score > self.config.medium_score_threshold:
            return self.config.max_branches_per_node
        else:
            return int(self.config.max_branches_per_node * 0.5)  # ä½åˆ†è·¯å¾„å°‘æ¢ç´¢

    def _try_merge_paths(self, new_path: Path, existing_paths: list[Path]) -> Path | None:
        """
        å°è¯•è·¯å¾„åˆå¹¶ï¼ˆç«¯ç‚¹ç›¸é‡ï¼‰

        Args:
            new_path: æ–°è·¯å¾„
            existing_paths: ç°æœ‰è·¯å¾„åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„è·¯å¾„ï¼Œå¦‚æœä¸åˆå¹¶åˆ™è¿”å› None
        """
        endpoint = new_path.get_leaf_node()
        if not endpoint:
            return None

        for existing in existing_paths:
            if existing.get_leaf_node() == endpoint and not existing.is_merged:
                # ç«¯ç‚¹ç›¸é‡ï¼Œåˆå¹¶è·¯å¾„
                merged_score = self._merge_score(new_path.score, existing.score)

                merged_path = Path(
                    nodes=new_path.nodes,  # ä¿ç•™æ–°è·¯å¾„çš„èŠ‚ç‚¹åºåˆ—
                    edges=new_path.edges,
                    score=merged_score,
                    depth=new_path.depth,
                    parent=new_path.parent,
                    is_merged=True,
                    merged_from=[new_path, existing],
                )

                # ä»ç°æœ‰åˆ—è¡¨ä¸­ç§»é™¤è¢«åˆå¹¶çš„è·¯å¾„
                existing_paths.remove(existing)

                logger.debug(f"ğŸ”€ è·¯å¾„åˆå¹¶: {new_path.score:.3f} + {existing.score:.3f} â†’ {merged_score:.3f}")

                return merged_path

        return None

    def _merge_score(self, score1: float, score2: float) -> float:
        """
        åˆå¹¶ä¸¤æ¡è·¯å¾„çš„åˆ†æ•°

        Args:
            score1: è·¯å¾„1åˆ†æ•°
            score2: è·¯å¾„2åˆ†æ•°

        Returns:
            åˆå¹¶ååˆ†æ•°
        """
        if self.config.path_merge_strategy == "weighted_geometric":
            # å‡ ä½•å¹³å‡ Ã— 1.2 åŠ æˆ
            return (score1 * score2) ** 0.5 * 1.2
        elif self.config.path_merge_strategy == "max_bonus":
            # å–æœ€å¤§å€¼ Ã— 1.3 åŠ æˆ
            return max(score1, score2) * 1.3
        else:
            # é»˜è®¤ï¼šç®—æœ¯å¹³å‡ Ã— 1.15 åŠ æˆ
            return (score1 + score2) / 2 * 1.15

    def _extract_leaf_paths(self, all_paths: list[Path]) -> list[Path]:
        """
        æå–å¶å­è·¯å¾„ï¼ˆæœ€å°å­è·¯å¾„ï¼‰

        Args:
            all_paths: æ‰€æœ‰è·¯å¾„

        Returns:
            å¶å­è·¯å¾„åˆ—è¡¨
        """
        # æ„å»ºçˆ¶å­å…³ç³»æ˜ å°„
        children_map: dict[int, list[Path]] = {}
        for path in all_paths:
            if path.parent:
                parent_id = id(path.parent)
                if parent_id not in children_map:
                    children_map[parent_id] = []
                children_map[parent_id].append(path)

        # æå–æ²¡æœ‰å­è·¯å¾„çš„è·¯å¾„
        leaf_paths = [p for p in all_paths if id(p) not in children_map]

        return leaf_paths

    async def _map_paths_to_memories(self, paths: list[Path]) -> dict[str, tuple[Any, list[Path]]]:
        """
        å°†è·¯å¾„æ˜ å°„åˆ°è®°å¿†

        Args:
            paths: è·¯å¾„åˆ—è¡¨

        Returns:
            {memory_id: (Memory, [contributing_paths])}
        """
        # ä½¿ç”¨ä¸´æ—¶å­—å…¸å­˜å‚¨è·¯å¾„åˆ—è¡¨
        temp_paths: dict[str, list[Path]] = {}
        temp_memories: dict[str, Any] = {}  # å­˜å‚¨ Memory å¯¹è±¡

        for path in paths:
            # æ”¶é›†è·¯å¾„ä¸­æ‰€æœ‰èŠ‚ç‚¹æ¶‰åŠçš„è®°å¿†
            memory_ids_in_path = set()

            for node_id in path.nodes:
                memory_ids = self.graph_store.node_to_memories.get(node_id, [])
                memory_ids_in_path.update(memory_ids)

            # å°†è·¯å¾„è´¡çŒ®ç»™æ‰€æœ‰æ¶‰åŠçš„è®°å¿†
            for mem_id in memory_ids_in_path:
                memory = self.graph_store.get_memory_by_id(mem_id)
                if memory:
                    if mem_id not in temp_paths:
                        temp_paths[mem_id] = []
                        temp_memories[mem_id] = memory
                    temp_paths[mem_id].append(path)

        # æ„å»ºæœ€ç»ˆè¿”å›çš„å­—å…¸
        memory_paths: dict[str, tuple[Any, list[Path]]] = {
            mem_id: (temp_memories[mem_id], paths_list)
            for mem_id, paths_list in temp_paths.items()
        }

        return memory_paths

    async def _final_scoring(
        self, memory_paths: dict[str, tuple[Any, list[Path]]]
    ) -> list[tuple[Any, float, list[Path]]]:
        """
        æœ€ç»ˆè¯„åˆ†ï¼ˆç»“åˆè·¯å¾„åˆ†æ•°ã€é‡è¦æ€§ã€æ—¶æ•ˆæ€§ + åå¥½ç±»å‹åŠ æˆï¼‰

        Args:
            memory_paths: è®°å¿†IDåˆ°(è®°å¿†, è·¯å¾„åˆ—è¡¨)çš„æ˜ å°„

        Returns:
            [(Memory, final_score, paths), ...]
        """
        scored_memories = []

        for mem_id, (memory, paths) in memory_paths.items():
            # 1. èšåˆè·¯å¾„åˆ†æ•°
            path_score = self._aggregate_path_scores(paths)

            # 2. è®¡ç®—é‡è¦æ€§åˆ†æ•°
            importance_score = memory.importance

            # 3. è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°
            recency_score = self._calculate_recency(memory)

            # 4. ç»¼åˆè¯„åˆ†
            weights = self.config.final_scoring_weights
            base_final_score = (
                path_score * weights["path_score"]
                + importance_score * weights["importance"]
                + recency_score * weights["recency"]
            )

            # ğŸ†• 5. åå¥½ç±»å‹åŠ æˆï¼ˆæ£€æŸ¥è®°å¿†ä¸­çš„èŠ‚ç‚¹ç±»å‹ï¼‰
            type_bonus = 0.0
            if self.prefer_node_types:
                # è·å–è®°å¿†ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
                memory_nodes = getattr(memory, "nodes", [])
                if memory_nodes:
                    # è®¡ç®—åŒ¹é…åå¥½ç±»å‹çš„èŠ‚ç‚¹æ¯”ä¾‹
                    matched_count = 0
                    for node in memory_nodes:
                        # ä» Node å¯¹è±¡æå– ID
                        node_id = node.id if hasattr(node, "id") else str(node)
                        node_data = await self.vector_store.get_node_by_id(node_id)
                        if node_data:
                            metadata = node_data.get("metadata", {})
                            node_type = metadata.get("node_type")
                            if node_type and node_type in self.prefer_node_types:
                                matched_count += 1
                    
                    if matched_count > 0:
                        match_ratio = matched_count / len(memory_nodes)
                        # æ ¹æ®åŒ¹é…æ¯”ä¾‹ç»™äºˆåŠ æˆï¼ˆæœ€é«˜10%ï¼‰
                        type_bonus = base_final_score * match_ratio * 0.1
                        logger.debug(
                            f"è®°å¿† {mem_id[:8]} åŒ…å« {matched_count}/{len(memory_nodes)} ä¸ªåå¥½ç±»å‹èŠ‚ç‚¹ï¼Œ"
                            f"åŠ æˆ {type_bonus:.3f}"
                        )

            final_score = base_final_score + type_bonus
            scored_memories.append((memory, final_score, paths))

        return scored_memories

    def _aggregate_path_scores(self, paths: list[Path]) -> float:
        """
        èšåˆå¤šæ¡è·¯å¾„çš„åˆ†æ•°

        Args:
            paths: è·¯å¾„åˆ—è¡¨

        Returns:
            èšåˆåˆ†æ•°
        """
        if not paths:
            return 0.0

        # æ–¹æ¡ˆA: æ€»åˆ†ï¼ˆè·¯å¾„è¶Šå¤šï¼Œåˆ†æ•°è¶Šé«˜ï¼‰
        total_score = sum(p.score for p in paths)

        # æ–¹æ¡ˆB: Top-Kå¹³å‡ï¼ˆå…³æ³¨æœ€ä¼˜è·¯å¾„ï¼‰
        top3 = sorted(paths, key=lambda p: p.score, reverse=True)[:3]
        avg_top = sum(p.score for p in top3) / len(top3) if top3 else 0.0

        # ç»„åˆï¼š40% æ€»åˆ† + 60% Topå‡åˆ†
        return total_score * 0.4 + avg_top * 0.6

    def _calculate_recency(self, memory: Any) -> float:
        """
        è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°

        Args:
            memory: è®°å¿†å¯¹è±¡

        Returns:
            æ—¶æ•ˆæ€§åˆ†æ•° [0, 1]
        """
        now = datetime.now(timezone.utc)

        # ç¡®ä¿æ—¶é—´æœ‰æ—¶åŒºä¿¡æ¯
        if memory.created_at.tzinfo is None:
            memory_time = memory.created_at.replace(tzinfo=timezone.utc)
        else:
            memory_time = memory.created_at

        # è®¡ç®—å¤©æ•°å·®
        age_days = (now - memory_time).total_seconds() / 86400

        # 30å¤©åŠè¡°æœŸ
        recency_score = 1.0 / (1.0 + age_days / 30)

        return recency_score


__all__ = ["PathScoreExpansion", "PathExpansionConfig", "Path"]
